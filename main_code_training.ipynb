{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "#importing machine learning libraries\n",
    "import tensorflow as tf\n",
    "# from tensorflow.kears.models import In\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#Secondary imports\n",
    "import pandas_profiling as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_file.csv')\n",
    "test = pd.read_csv('test_file.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pp_train = pp.ProfileReport(train)\n",
    "pp_test = pp.ProfileReport(test)\n",
    "\n",
    "pp_train.to_file('train_analysis.html')\n",
    "pp_test.to_file('test_analysis.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCOUNT NUMBER',\n",
       " 'ADDRESS',\n",
       " 'APPLICATION CREATED DATE',\n",
       " 'APPLICATION REQUIREMENTS COMPLETE',\n",
       " 'APPLICATION TYPE',\n",
       " 'CITY',\n",
       " 'CONDITIONAL APPROVAL',\n",
       " 'DATE ISSUED',\n",
       " 'DOING BUSINESS AS NAME',\n",
       " 'ID',\n",
       " 'LATITUDE',\n",
       " 'LEGAL NAME',\n",
       " 'LICENSE APPROVED FOR ISSUANCE',\n",
       " 'LICENSE CODE',\n",
       " 'LICENSE DESCRIPTION',\n",
       " 'LICENSE ID',\n",
       " 'LICENSE NUMBER',\n",
       " 'LICENSE STATUS',\n",
       " 'LICENSE STATUS CHANGE DATE',\n",
       " 'LICENSE TERM EXPIRATION DATE',\n",
       " 'LICENSE TERM START DATE',\n",
       " 'LOCATION',\n",
       " 'LONGITUDE',\n",
       " 'PAYMENT DATE',\n",
       " 'POLICE DISTRICT',\n",
       " 'PRECINCT',\n",
       " 'SITE NUMBER',\n",
       " 'SSA',\n",
       " 'STATE',\n",
       " 'WARD',\n",
       " 'WARD PRECINCT',\n",
       " 'ZIP CODE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing feature processing as discussed in my_approach.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''So the set of features I'll be using from the first analysis (mentioned in the approach file) will be, ['SITE_NUMBER', 'CITY', 'STATE', 'WARD', \n",
    " 'LICENSE_CODE', 'LICENSE_DESCRIPTION', 'LICENSE_TERM_START_DATE' - 'LICENSE_TERM_EXPIRATION_DATE' in days], \n",
    "Later I might use other variables as well'''\n",
    "\n",
    "\n",
    "features = ['SITE NUMBER', 'CITY', 'STATE', 'LICENSE CODE','LICENSE DESCRIPTION', 'LICENSE_DURATION', 'LICENSE_CHANGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding duration of license, as it's probably an important factor\n",
    "\n",
    "train['LICENSE_DURATION'] = list(map(lambda x, y: (x - y).days, pd.to_datetime(train['LICENSE TERM EXPIRATION DATE']), pd.to_datetime(train['LICENSE TERM START DATE'])))\n",
    "test['LICENSE_DURATION'] = list(map(lambda x, y: (x - y).days, pd.to_datetime(test['LICENSE TERM EXPIRATION DATE']), pd.to_datetime(test['LICENSE TERM START DATE'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "base = list(map(lambda x: str(x),pd.to_datetime(train['LICENSE STATUS CHANGE DATE']).values ))\n",
    "\n",
    "\n",
    "for i in range(len(base)):\n",
    "    if base[i] !='NaT':\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)\n",
    "    \n",
    "    \n",
    "train['LICENSE_CHANGE'] = temp\n",
    "\n",
    "temp = [] #If the license was changed, it's 1 else 0\n",
    "base = list(map(lambda x: str(x),pd.to_datetime(test['LICENSE STATUS CHANGE DATE']).values ))\n",
    "\n",
    "\n",
    "for i in range(len(base)):\n",
    "    if base[i] !='NaT':\n",
    "        temp.append(1)\n",
    "    else:\n",
    "        temp.append(0)\n",
    "    \n",
    "    \n",
    "test['LICENSE_CHANGE'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl1 = LabelEncoder()\n",
    "lbl2 = LabelEncoder()\n",
    "lbl3 = LabelEncoder()\n",
    "lbl4 = LabelEncoder()\n",
    "\n",
    "\n",
    "lbl1.fit(pd.concat((train['LICENSE DESCRIPTION'], test['LICENSE DESCRIPTION']), axis=0))\n",
    "train['LICENSE DESCRIPTION'] = lbl1.transform(train['LICENSE DESCRIPTION'])\n",
    "test['LICENSE DESCRIPTION'] = lbl1.transform(test['LICENSE DESCRIPTION'])\n",
    "                                              \n",
    "lbl2.fit(pd.concat((train['CITY'], test['CITY']), axis=0))\n",
    "train['CITY'] = lbl2.transform(train['CITY'])\n",
    "test['CITY'] = lbl2.transform(test['CITY'])\n",
    "                                              \n",
    "                                              \n",
    "lbl3.fit(pd.concat((train['STATE'], test['STATE']), axis=0))\n",
    "train['STATE'] = lbl3.transform(train['STATE'])\n",
    "test['STATE'] = lbl3.transform(test['STATE'])\n",
    "                                              \n",
    "lbl4.fit(train['LICENSE STATUS'])\n",
    "train['LICENSE STATUS'] = lbl4.transform(train['LICENSE STATUS'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if POLICE DISTRICT is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is when POLICE DISTRICT is not null (array([0, 1, 3, 4]), array([24830,  6768,     3,   282]))\n",
      "This is when POLICE DISTRICT is null (array([0, 1, 2, 4]), array([ 5370, 48632,     2,     8]))\n"
     ]
    }
   ],
   "source": [
    "print(\"This is when POLICE DISTRICT is not null\",np.unique(train[~train['POLICE DISTRICT'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print(\"This is when POLICE DISTRICT is null\", np.unique(train[train['POLICE DISTRICT'].isna()]['LICENSE STATUS'].values, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAI'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl4.inverse_transform([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append('POLICE DISTRICT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, the LICENSE STATUS of AAI was significantly higher when POLICE DISTRICT was null, next highest term is AAC which is higher when POLICE DISTRICT is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['POLICE DISTRICT'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if WARD and WARD PRECINCT is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is when WARD is not null (array([0, 1, 3, 4]), array([25808, 10094,     3,   289]))\n",
      "This is when WARD is null (array([0, 1, 2, 4]), array([ 4392, 45306,     2,     1]))\n",
      "\n",
      "This is when WARD PRECINCT is not null (array([0, 1, 3, 4]), array([25809, 10094,     3,   289]))\n",
      "This is when WARD PRECINCT is null (array([0, 1, 2, 4]), array([ 4391, 45306,     2,     1]))\n"
     ]
    }
   ],
   "source": [
    "# Checking for WARD first\n",
    "print(\"This is when WARD is not null\",np.unique(train[~train['WARD'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print(\"This is when WARD is null\", np.unique(train[train['WARD'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print()\n",
    "# Checking for WARD  PRECINCT first\n",
    "print(\"This is when WARD PRECINCT is not null\",np.unique(train[~train['WARD PRECINCT'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print(\"This is when WARD PRECINCT is null\", np.unique(train[train['WARD PRECINCT'].isna()]['LICENSE STATUS'].values, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### They both show very similar effect on LICENSE STATUS, and the results are also very similar to POLICE DISTRICT (as observed in correlation matrix), so not taking these features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if LATITUDE, LONGITUDE and LOCATION is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is when LATITUDE is not null (array([0, 1, 3, 4]), array([25968, 12390,     3,   288]))\n",
      "This is when LATITUDE is null (array([0, 1, 2, 4]), array([ 4232, 43010,     2,     2]))\n",
      "\n",
      "This is when LONGITUDE is not null (array([0, 1, 3, 4]), array([25968, 12390,     3,   288]))\n",
      "This is when LONGITUDE is null (array([0, 1, 2, 4]), array([ 4232, 43010,     2,     2]))\n",
      "\n",
      "This is when LOCATION is not null (array([0, 1, 3, 4]), array([25968, 12390,     3,   288]))\n",
      "This is when LOCATION is null (array([0, 1, 2, 4]), array([ 4232, 43010,     2,     2]))\n"
     ]
    }
   ],
   "source": [
    "# Checking for LATITUDE first\n",
    "print(\"This is when LATITUDE is not null\",np.unique(train[~train['LATITUDE'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print(\"This is when LATITUDE is null\", np.unique(train[train['LATITUDE'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print()\n",
    "# Checking for LONGITUDE first\n",
    "print(\"This is when LONGITUDE is not null\",np.unique(train[~train['LONGITUDE'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print(\"This is when LONGITUDE is null\", np.unique(train[train['LONGITUDE'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "\n",
    "print()\n",
    "# Checking for LOCATION first\n",
    "print(\"This is when LOCATION is not null\",np.unique(train[~train['LOCATION'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print(\"This is when LOCATION is null\", np.unique(train[train['LOCATION'].isna()]['LICENSE STATUS'].values, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These three features show very similar effect on LICENSE STATUS, and the results are also very similar to POLICE DISTRICT (as observed in correlation matrix), so not taking these features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if DOING BUSINESS AS NAME is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is when DOING BUSINESS AS NAME is not null (array([0, 1, 2, 3, 4]), array([30199, 55400,     2,     3,   290]))\n",
      "This is when DOING BUSINESS AS NAME is null (array([0]), array([1]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for DOING BUSINESS AS NAME first\n",
    "print(\"This is when DOING BUSINESS AS NAME is not null\",np.unique(train[~train['DOING BUSINESS AS NAME'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print(\"This is when DOING BUSINESS AS NAME is null\", np.unique(train[train['DOING BUSINESS AS NAME'].isna()]['LICENSE STATUS'].values, return_counts=True))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can add this to our feature set, but it has many unique values, and considering the size of dataset, it's not recommended to use label encoder for 50k unique values, with dataset of size 80k, so we're ignoring it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using number of days between DATE ISSUED and LICENSE START DATE as a feature, as it might have some latent information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['INTERVAL'] = list(map(lambda x, y: (x - y).days, pd.to_datetime(train['DATE ISSUED']), pd.to_datetime(train['LICENSE TERM START DATE'])))\n",
    "test['INTERVAL'] =  list(map(lambda x, y: (x - y).days, pd.to_datetime(test['DATE ISSUED']), pd.to_datetime(test['LICENSE TERM START DATE'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1334.,  208.,    0.,    0.,   20.,    0.,  365.,   -9.,    0.,\n",
       "        -23.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['INTERVAL'].values[:10]\n",
    "\n",
    "#As we can see, there are some negative values. Which indicates some of the licenses were valid before they were offically issued, which might prove useful\n",
    "#depending on domain information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SITE NUMBER',\n",
       " 'CITY',\n",
       " 'STATE',\n",
       " 'LICENSE CODE',\n",
       " 'LICENSE DESCRIPTION',\n",
       " 'LICENSE_DURATION',\n",
       " 'LICENSE_CHANGE',\n",
       " 'POLICE DISTRICT',\n",
       " 'INTERVAL',\n",
       " 'INTERVAL']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.append('INTERVAL')\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally creating training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx, testx, trainy, testy = train_test_split(train[features].values, train['LICENSE STATUS'].values, test_size = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting construction of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbb = lgb.LGBMClassifier()\n",
    "lgbb.fit(trainx, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/lgb.pkl','wb+') as handle:\n",
    "    pickle.dump(lgbb, handle)\n",
    "    \n",
    "with open('tmp/lbl1.pkl','wb+') as handle:\n",
    "    pickle.dump(lbl1, handle)\n",
    "with open('tmp/lbl2.pkl','wb+') as handle:\n",
    "    pickle.dump(lbl2, handle)\n",
    "with open('tmp/lbl3.pkl','wb+') as handle:\n",
    "    pickle.dump(lbl3, handle)\n",
    "with open('tmp/lbl4.pkl','wb+') as handle:\n",
    "    pickle.dump(lbl4, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lgbb.predict(test[features])\n",
    "\n",
    "np.unique(result, return_counts=True)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['ID'] = test['ID']\n",
    "submission['LICENSE STATUS'] = lbl4.inverse_transform(result)\n",
    "\n",
    "submission.head()\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
